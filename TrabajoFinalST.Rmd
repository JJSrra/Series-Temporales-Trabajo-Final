---
title: 'Series Temporales: Trabajo Final'
author: "Juanjo Sierra"
date: "22 de abril de 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Paquetes a cargar

Importamos los paquetes que necesitamos para resolver los problemas planteados para la práctica.

```{r}
library(tseries)
library(dplyr)
```

## Problema 1

Teniendo los datos de una estación meteorológica (ubicada en Loja) desde mayo de 2013 hasta febrero de 2018, se pide predecir los valores de temperatura máxima mensuales para los meses de marzo y abril de 2018.

En primer lugar, se leen los datos de la estación meteorológica escogida.

```{r}
datosEstacion = read.csv("5582A.csv", sep = ";")
summary(datosEstacion)
```

Como solamente vamos a trabajar con la temperatura máxima y necesitamos agruparlas por fecha, nos quedamos únicamente con esas dos columnas.

```{r}
# Nos quedamos con los valores de fecha y temperatura máxima
datosEstacion = datosEstacion[,2:3]
head(datosEstacion)
```

Como hemos comprobado antes en el summary que sí que hay valores perdidos (122), vamos a eliminar las instancias en las que haya un NA para poder calcular correctamente cuál es la temperatura máxima en cada mes.

```{r}
# Elimino aquellos datos que sean NA
datosEstacionSinNA = datosEstacion[-which(is.na(datosEstacion$Tmax)),]

# Comprobamos la nueva dimensionalidad de los datos
# y confirmamos que ya no hay datos perdidos
dim(datosEstacionSinNA)
anyNA(datosEstacionSinNA)
```

Hemos reducido la dimensionalidad de los datos de 1726 a 1604, pero ya no hay ningún valor perdido y podemos agrupar los datos por mes, obteniendo la temperatura máxima en cada uno.

Para obtener dichos datos máximos utilizo la librería `dplyr`.

```{r}
# Almacenamos la variable Fecha como tipo Date
datosEstacionSinNA$Fecha = as.Date(datosEstacionSinNA$Fecha)

# Convertimos el dataset en dos columnas (Fecha y Mes),
# agrupamos por mes y año y para todos los valores nos
# quedamos con el valor máximo de TMax
datosTmaxMensual = datosEstacionSinNA %>%
mutate(Mes = format(Fecha, "%m"), Año = format(Fecha, "%Y")) %>%
group_by(Año, Mes) %>%
summarise(Tmax = max(Tmax))
```

Ahora podemos trabajar con la columna TMax como nuestra serie temporal. Podemos crear el objeto "Serie Temporal" con la librería `tseries`. Usando `plot` y `decompose` se pueden echar un vistazo general al aspecto de nuestros datos. Incluimos un valor de `frequency` de 12 porque es lo que estimamos que es el periodo de la estacionalidad (de año en año y tenemos valores mensuales).

```{r}
# Observamos la tendencia y estacionalidad con el decompose
# y utilizamos frecuencia 12 porque asumimos que tienen
# estacionalidad anual
serie = datosTmaxMensual$Tmax
serie.ts = ts(serie, frequency=12)
plot(decompose(serie.ts))
```

Buscando que la serie sea estacionaria, implicando eso que no varíe ni en media ni en varianza, vamos a realizar una transformación logarítmica para conseguir suavizar los valores.

```{r}
# Realizamos una transformación logarítmica
# y volvemos a mostrar los valores del decompose
serie.ts.log = log(serie.ts)
serie.log = log(serie)
plot(decompose(serie.ts.log))
```

Ya podemos empezar a trabajar con los datos. Como primer paso, vamos a asumir que todo el conjunto de train es el total de datos que tenemos y que la predicción a realizar estará compuesta por 2 valores (marzo y abril de 2018).

```{r}
nPred = 2
serie.train = serie.log
tiempo.train = 1:length(serie.train)
tiempo.pred = (length(tiempo.train)+1):(length(tiempo.train)+nPred)
```

A continuación tenemos que estimar la tendencia. Vamos a utilizar un modelo lineal simple dado que basándonos en la gráfica puede generalizar aceptablemente. Construimos el modelo lineal con la función `lm`.

```{r}
parametros.lm = lm(serie.train ~ tiempo.train)

tendencia.train = parametros.lm$coefficients[1]+tiempo.train*parametros.lm$coefficients[2]
tendencia.pred = parametros.lm$coefficients[1]+tiempo.pred*parametros.lm$coefficients[2]
```

En la siguiente gráfica mostramos la tendencia estimada en la misma gráfica que la serie temporal.

```{r}
plot.ts(serie.train, xlim=c(1, tiempo.pred[length(tiempo.pred)]))
lines(tiempo.train, tendencia.train, col="blue")
lines(tiempo.pred, tendencia.pred, col="green")
```

Para validar que el modelo es correcto, dado que no se puede afirmar con la gráfica que hemos obtenido, utilizaremos el test de Jarque Bera sobre los residuos que han quedado de generar el modelo lineal.

```{r}
JB.train = jarque.bera.test(parametros.lm$residuals)
JB.train
```

El test de Jarque Bera da un p-value de 0.18, que está por encima de 0.05. Por este motivo, no tenemos suficiente confianza como para afirmar que los residuos no sigan una distribución normal, y por tanto asumimos que sí que la siguen.

El siguiente paso es eliminar la tendencia a la serie. Comprobaremos en una gráfica qué aspecto tiene una vez eliminada esa tendencia.

```{r}
serie.train.sinTend = serie.train - tendencia.train
plot.ts(serie.train.sinTend, xlim=c(1, tiempo.train[length(tiempo.train)]))
```

Se puede observar que la gráfica tiene el mismo aspecto, solamente se ha trasladado en el eje Y, ubicándose ahora en torno al 0.

El siguiente paso es eliminar la estacionalidad del modelo. Inicialmente supusimos una estacionalidad de frecuencia 12 (anual).

```{r}
k = 12
estacionalidad = decompose(serie.ts.log)$seasonal[1:k]
estacionalidad
```

Aquí vemos los 12 valores que componen la estacionalidad de la serie. Para eliminar la estacionalidad hay que restar estos valores de forma periódica a lo largo de la serie.

```{r}
# Aprovecho el reciclaje de R para no tener que crear una variable auxiliar
serie.train.sinTendSinEst = serie.train.sinTend - estacionalidad

plot.ts(serie.train.sinTendSinEst, xlim=c(1, tiempo.train[length(tiempo.train)]))
```

Esta serie ya no tiene tendencia ni estacionalidad, como íbamos buscando. Ahora queremos comprobar si es estacionaria. Para ello podemos comenzar por realizar un test ACF.

```{r}
acf(serie.train.sinTendSinEst)
```

En la gráfica podemos observar que los valores descienden muy rápido y continúan por la serie siguiendo un formato de "olas", por lo que tiene toda la pinta de ser un modelo autorregresivo. Para asegurar que se trata de una serie estacionaria vamos a realizar el test de Dickey-Fuller.

```{r}
adf.serie.train = adf.test(serie.train.sinTendSinEst)
adf.serie.train
```

Según los resultados del test de Dickey-Fuller, tenemos una confianza de en torno al 98% de que la serie sea estacionaria. Debido a que hemos observado este comportamiento de "olas" en el test ACF podemos sugerir que se trata de un modelo autorregresivo, pero podemos comprobar en un test PACF si también puede tratarse de un modelo de medias móviles.

```{r}
pacf(serie.train.sinTendSinEst)
```

En esta gráfica se intuye un comportamiento similar pero sin embargo los valores más lejanos al 0 se encuentran muy próximos al umbral y no se produce un descenso tan rápido y tan grande en estos valores. Por lo tanto, propongo generar un modelo autorregresivo de grado 0 (el último instante de tiempo en el que el PACF pasa del umbral) y sin diferenciación.

```{r}
modelo = arima(serie.train.sinTendSinEst, order = c(0,0,0))
```

Una vez construido el modelo podemos generar las predicciones para los valores que buscamos (los 2 siguientes, marzo y abril).

```{r}
predicciones = predict(modelo, n.ahead = nPred)
valoresPredichos = predicciones$pred
valoresPredichos
```

Podemos observar los valores predichos con el resto de los valores de la serie transformada.

```{r}
plot.ts(serie.train.sinTendSinEst, xlim=c(1, tiempo.pred[length(tiempo.pred)]))
lines(tiempo.pred, valoresPredichos, col="red")
```

Se aprecia que al menos los valores siguen la tendencia que parecía intuirse de los últimos valores (hay un pico que está decayendo en los últimos meses).

Para comprobar que este modelo sea válido vamos a aplicar el test de Box-Pierce para comprobar que los residuos que quedan del modelo siguen una distribución aleatoria.

```{r}
boxPierce.test = Box.test(modelo$residuals)
boxPierce.test
```

Según el p-value que refleja el test de Box-Pierce los residuos no siguen una distribución aleatoria, es decir, hay al menos una parte de la serie que se puede modelar mediante algo que no sea aleatorio. Por este motivo vamos a volver a las gráficas ACF y PACF y vamos a comprobar que, de considerar un modelo de medias móviles en lugar de uno autorregresivo, podríamos generar un modelo ARIMA(0,0,1). Probemos si con este modelo las predicciones y los residuos que nos quedan se adaptan a lo que buscamos.

```{r}
modelo = arima(serie.train.sinTendSinEst, order = c(0,0,1))

predicciones = predict(modelo, n.ahead = nPred)
valoresPredichos = predicciones$pred
valoresPredichos

plot.ts(serie.train.sinTendSinEst, xlim=c(1, tiempo.pred[length(tiempo.pred)]))
lines(tiempo.pred, valoresPredichos, col="red")
```

A primera vista esta aproximación también podría encajar. Vamos a probar el test de Box-Pierce.

```{r}
boxPierce.test = Box.test(modelo$residuals)
boxPierce.test
```

Este test ya sí nos asegura que los residuos son aleatorios, por lo que el modelo se adapta mejor a la serie. Con los tests de Jarque Bera y Shapiro-Wilk probamos si los residuos siguen una distribución normal.

```{r}
jarqueBera.test = jarque.bera.test(modelo$residuals)
jarqueBera.test

shapiroWilk.test = shapiro.test(modelo$residuals)
shapiroWilk.test
```

Los p-values tan altos que dan estos tests no dan lugar a pensar que los residuos puedan seguir una distribución no normal. Por tanto, estos residuos también se ajustan a lo que buscamos y podemos asumir que el modelo es bueno. Podemos mostrar un histograma con la distribución de los residuos para verlo gráficamente. 

```{r}
hist(modelo$residuals, col="blue", prob=TRUE, ylim=c(0,5), xlim=c(-0.5,0.5))
lines(density(modelo$residuals))
```

Para obtener los datos finales de temperatura hay que deshacer sobre los datos predichos las transformaciones que hemos realizado antes.

```{r}
# Sumamos estacionalidad
# De nuevo aprovecho el reciclaje de R
# Como el vector de estacionalidad empieza en mayo
# hay que especificar que el valor que sumamos es el que
# le corresponde a marzo y abril
valoresPredichos.Est = valoresPredichos + estacionalidad[11:12]

# Sumamos tendencia
valoresPredichos.EstTend = valoresPredichos.Est + tendencia.pred

# Deshacemos transformación logarítmica
valoresPredichos.EstTendExp = exp(valoresPredichos.EstTend)
```

Por último, mostramos los datos predichos junto a los que hemos utilizado para entrenar, de forma que continúen la serie.

```{r}
plot.ts(serie, xlim=c(1, tiempo.pred[length(tiempo.pred)]))
lines(tiempo.pred, valoresPredichos.EstTendExp, col="red")
```

Los valores finales predichos por el modelo para los meses de marzo y abril son los siguientes.

```{r}
valoresPredichos.EstTendExp
```

## Problema 2

Teniendo los datos de una estación meteorológica (ubicada en Loja) desde mayo de 2013 hasta febrero de 2018, de forma diaria, se pide predecir los valores de temperatura máxima por día en la primera semana de marzo de 2018.

Partiendo de los datos que tenemos de la estación, solamente sabiendo la fecha y la temperatura máxima, imputamos los valores perdidos para poder tener una serie continua.

```{r}
library(imputeTS)

datosEstacionImp = na.interpolation(datosEstacion)
anyNA(datosEstacionImp)
```

Una vez imputados los datos analizamos la nueva serie de datos diarios con la función `decompose`. Esta vez, al tratar con datos diarios, la frecuencia de la estacionalidad será 365.

```{r}
serie = datosEstacionImp$Tmax
serie.ts = ts(serie, frequency=365)
plot(decompose(serie.ts))
```

Para suavizar los datos de la componente aleatoria y acercar su media al valor 0, vamos a realizar una transformación logarítmica a la serie.

```{r}
# Realizamos una transformación logarítmica
# y volvemos a mostrar los valores del decompose
serie.ts.log = log(serie.ts)
serie.log = log(serie)
plot(decompose(serie.ts.log))
```

Ahora los datos se aproximan al 0 y la media saldrá muy próxima a ese valor. Está más próxima a ser estacionaria.

Vamos a calcular los datos a predecir y a definir la serie que usaremos como train (todos los valores). El número de predicciones será 7 (los 7 días de la primera semana de marzo).

```{r}
nPred = 7
serie.train = serie.log
tiempo.train = 1:length(serie.train)
tiempo.pred = (length(tiempo.train)+1):(length(tiempo.train)+nPred)
```

El siguiente paso es estimar la tendencia de la serie.

```{r}
parametros.lm = lm(serie.train ~ tiempo.train)

tendencia.train = parametros.lm$coefficients[1]+tiempo.train*parametros.lm$coefficients[2]
tendencia.pred = parametros.lm$coefficients[1]+tiempo.pred*parametros.lm$coefficients[2]
```

En la siguiente gráfica mostramos la tendencia estimada en la misma gráfica que la serie temporal.

```{r}
plot.ts(serie.train, xlim=c(1, tiempo.pred[length(tiempo.pred)]))
lines(tiempo.train, tendencia.train, col="blue")
lines(tiempo.pred, tendencia.pred, col="green")
```

Vamos a utilizar el test de Jarque Bera sobre los residuos que han quedado de generar el modelo lineal, para comprobar si siguen una distribución normal.

```{r}
JB.train = jarque.bera.test(parametros.lm$residuals)
JB.train
```

Como el test de Jarque Bera no indica que los residuos sigan una distribución normal, vamos a probar un nuevo modelo que haga transformaciones al atributo tiempo.

```{r}
parametros.lm = lm(serie.train ~ tiempo.train + I(tiempo.train^2))

tendencia.train = parametros.lm$coefficients[1]+tiempo.train*parametros.lm$coefficients[2]
tendencia.pred = parametros.lm$coefficients[1]+tiempo.pred*parametros.lm$coefficients[2]

plot.ts(serie.train, xlim=c(1, tiempo.pred[length(tiempo.pred)]))
lines(tiempo.train, tendencia.train, col="blue")
lines(tiempo.pred, tendencia.pred, col="green")

JB.train = jarque.bera.test(parametros.lm$residuals)
JB.train
```

Como esto tampoco funciona bien, probamos a realizar una serie de diferenciaciones a la serie para intentar suavizar la tendencia.

```{r}
serieDif = diff(serie.train)
tiempoDif = tiempo.train[2:length(tiempo.train)]
tiempoPredDif = (tiempoDif[length(tiempoDif)]+1):(tiempoDif[length(tiempoDif)] + nPred)
parametros.lm = lm(serieDif ~ tiempoDif)

tendencia.train = parametros.lm$coefficients[1]+tiempoDif*parametros.lm$coefficients[2]
tendencia.pred = parametros.lm$coefficients[1]+tiempoPredDif*parametros.lm$coefficients[2]

plot.ts(serieDif, xlim=c(1, tiempoPredDif[length(tiempoPredDif)]))
lines(tiempoDif, tendencia.train, col="blue")
lines(tiempoPredDif, tendencia.pred, col="green")

JB.train = jarque.bera.test(parametros.lm$residuals)
JB.train
```

Como el test de Jarque Bera sigue negando que los residuos sigan una distribución normal no podemos asumir que el modelo estime la tendencia correctamente, y además al ser esta tendencia calculada casi constante, no voy a eliminarla para los siguientes pasos.

El siguiente paso es eliminar la estacionalidad. En este caso como hemos dicho anteriormente, se entiende una estacionalidad de frecuencia 365 (anual contando en días).

```{r}
k = 365
estacionalidad = decompose(serie.ts.log)$seasonal[1:k]
```

Para restar esta estacionalidd hay que hacerlo de forma periódica a lo largo de toda la serie.

```{r}
# Aprovecho el reciclaje de R para no tener que crear una variable auxiliar
serie.train.SinEst = serie.train - estacionalidad

plot.ts(serie.train.SinEst, xlim=c(1, tiempo.train[length(tiempo.train)]))
```

Ahora que la serie ya no tiene estacionalidad y hemos analizado que no teníamos que restarle la tendencia, vamos a comprobar si es estacionaria. Para ello podemos probar en principio a ejecutar un test ACF y un PACF.

```{r}
acf(serie.train.SinEst)
pacf(serie.train.SinEst)
```

En el gráfico del test PACF se puede observar un rápido descenso, pero no se distingue un claro comportamiento de "olas". Por otro lado, en la gráfica del test ACF ni siquiera desciende rápido y no se queda por debajo del 0, por lo que es un comportamiento extraño. Para comprobar estadísticamente si se trata de una serie estacionaria podemos ejecutar el test de Dickey-Fuller aumentado y obtener nuestras conclusiones a partir de ello.

```{r}
adf.test(serie.train.SinEst)
```

Según el resultado arrojado por el test, el p-value bajo refleja una confianza alta en que la serie es estacionaria, por tanto se puede tratar de generar un modelo ARIMA. Al ser PACF el que parece que sigue una forma más de "olas", vamos a asumir que se trate de un modelo de medias móviles, cuyo grado podría ser cuestionable dado que todos los valores del test ACF se encuentran por encima del umbral. Voy a escoger el orden 3 porque es el último valor que se encuentra rondando el 0.5.

```{r}
modelo = arima(serie.train.SinEst, order = c(0,0,11))
```

Una vez construido el modelo podemos generar las predicciones para los valores que buscamos (la primera semana de marzo).

```{r}
predicciones = predict(modelo, n.ahead = nPred)
valoresPredichos = predicciones$pred
valoresPredichos
```

Podemos observar los valores predichos con el resto de los valores de la serie transformada. Voy a mostrar sólo los últimos datos para que se aprecien bien los valores calculados.

```{r}
plot.ts(serie.train.SinEst, xlim=c(1500, tiempo.pred[length(tiempo.pred)]))
lines(tiempo.pred, valoresPredichos, col="red")
```

Los datos parecen coherentes, pero vamos a aplicar el test de Box-Pierce para asegurarnos que los residuos resultantes del modelo sean aleatorios.

```{r}
boxPierce.test = Box.test(modelo$residuals)
boxPierce.test
```

Este test ya sí nos asegura que los residuos son aleatorios, por lo que el modelo se adapta mejor a la serie. Con los tests de Jarque Bera y Shapiro-Wilk probamos si los residuos siguen una distribución normal.

```{r}
jarqueBera.test = jarque.bera.test(modelo$residuals)
jarqueBera.test

shapiroWilk.test = shapiro.test(modelo$residuals)
shapiroWilk.test
```
